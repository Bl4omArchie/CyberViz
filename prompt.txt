Your program crashes because the CSV files are being loaded into memory all at once, which exceeds your 8GB RAM capacity. Here are ways to optimize it:
1. Use Smaller Chunks with pandas

Problem: You load the entire dataset into memory:

chunks = pd.read_csv(self.filepath, low_memory=False, chunksize=20000)
df = pd.concat(chunks)

Fix: Process and save chunks one at a time instead of concatenating all chunks:

# Replace in `read_content` for CSV handling
if not os.path.exists(f"feather/{self.filename}.feather"):
    for i, chunk in enumerate(pd.read_csv(self.filepath, low_memory=False, chunksize=20000)):
        chunk.to_feather(f"feather/{self.filename}_part{i}.feather")
return pd.concat([pd.read_feather(f"feather/{self.filename}_part{i}.feather") for i in range(len(chunks))])

2. Optimize Data Types

Reduce memory usage by specifying appropriate dtypes during CSV reading.

Example:

pd.read_csv(self.filepath, dtype={
    'column1': 'float32', 
    'column2': 'int8', 
    'column3': 'category'
})

Analyze your dataset to determine which columns can use smaller types like int8, float32, or category.
3. Memory-efficient Storage

Use formats like Parquet or Feather instead of CSV for faster and memory-efficient storage.

df.to_parquet(f"feather/{self.filename}.parquet")
return pd.read_parquet(f"feather/{self.filename}.parquet")

4. On-Disk Operations

If you don’t need to load the entire dataset, use on-disk operations like filtering or aggregation during chunk reading.

for chunk in pd.read_csv(self.filepath, chunksize=20000):
    filtered_chunk = chunk[chunk['some_column'] > threshold]
    filtered_chunk.to_feather(f"feather/{self.filename}_filtered.feather", mode='a')

5. Increase Virtual Memory

If optimizing code isn't sufficient, increase your system's swap/virtual memory to avoid crashes.
You said:
can I use parquet and feather ? or there have the same purpose ?
ChatGPT said:
ChatGPT

Both Parquet and Feather are columnar storage formats, and while they share similar purposes, they have differences that might affect your choice depending on your use case.
Key Differences
Feature	Feather	Parquet
Performance	Faster for read/write operations	Slightly slower than Feather
Compression	No built-in compression (larger files)	Built-in compression (smaller files)
Compatibility	Designed for fast local operations	Better for distributed systems like Spark/Hive
File Size	Larger (no compression)	Smaller (compressed)
Use Case	Ideal for local data manipulation	Ideal for storage and large-scale processing
When to Use Feather

    Fast local reads/writes for iterative tasks in Python.
    Smaller datasets where disk space isn’t critical.

When to Use Parquet

    Need compressed files for efficient storage.
    Working with distributed systems like Hadoop, Spark, or AWS Athena.
    Large-scale processing or storage.

Can I Use Both?

Yes, you can use both:

    Use Feather for speed when preprocessing data locally.
    Use Parquet for storing processed data efficiently.

For your case with limited RAM, Feather may be a better choice during development because of its speed. Later, convert to Parquet if disk space becomes a concern.